7-1 Noise-robust 학습 방법론
-데이터에 노이즈가 있다면 어떤 결과?
Least Square는 아웃라이어에 취약, 아웃라이어 무시 못함
	상대적으로 에러 큰 값을 줄이려 하기 때문에

해결책 : L1-norm
	Robust Regression : 절대값(제곱 루트)
		줄여야 되는 로스값 자체 동일, 틀린 값 갖는 적은 개수 샘플 무시
	
	but 절대값 바로 활용 어려움 : 에러 0일때 기울기 정의 불가
	
	해결책 : Huber Loss : 엡실론 보다 작을때 Least Square 에러 / 클때 Absolute 에러 사용
		모든 영역 미분 가능하고, 아웃라이어에 강인함

-노이즈에 강한 학습 방법은?
Infinite Norm Regression : 노이즈가 있는 데이터에 관심 -> 에러값의 최대값 최소화

-RANSAC 알고리즘이란?
노이즈가 꽤 많은 비중으로 존재하는 경우 아웃라이어의 정보 무시 (인라이어 개수 최대인 모델)
처음 주어진 학습 데이터 일부 선택 -> 리니어 리그레이션 모델 계산 -> 인라이어 아웃라이너 구분 -> 반복


7-2 Overfitting과 Regularization
-Multi-class 문제 해결
클래스 세개 이상 : one vs all 문제
1번 클래스와 나머지, 2번 클래스와 나머지, 3번 클래스와 나머지 -> 가장 높은 점수 구함

-p-norm이란?
L2-norm : 어떤 두 개의 피쳐 존재할 때 그 둘의 기하학상 거리
L1 : 아웃라이어에 강함
L0 : 0 아닌 것의 개수 셈

Frobenius Norm : 매트릭스의 형태에 대해서 norm 값 얻는 방법

-Regularization이란?
오버피팅 : 학습 데이터에 대해서만 성능이 좋고 테스트 상 성능 떨어짐
	특정 w 커짐 -> 해당 피쳐만 중심적으로 보고 있음

해결 : L2-Regularization : Least Square에 특정 w 커지지 않도록 조절
