{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_텍스트_전처리.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPc03neDuqwwivaTNbQAYfU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 02 텍스트 전처리\n",
        "### 01) 토큰화 Tokenization\n",
        "1. 단어 토큰화 : 보통은 띄어쓰기 단위지만 구두점이나 특수문자도 고려해야 함\n",
        "2. 토큰화 중 생기는 선택의 순간 : don't'''"
      ],
      "metadata": {
        "id": "q37JaPq74rMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "# word_tokenize : Don't -> Do, n't / Jone's -> Jone, 's\n",
        "\n",
        "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "# wordPuncTokenizer : Don't -> Don, ', t\n",
        "\n",
        "print('단어 토큰화3 :',text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "# keras text_to_word_sequence : don't -> don't"
      ],
      "metadata": {
        "id": "eCynpJQD5pP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 토큰화 고려 사항\n",
        "- 구둣점이나 특수문자 단순 제외 불가\n",
        "- 줄임말과 단어 내 띄어쓰기 \n",
        "\n",
        "4. 문장 토큰화 : 한국어 KSS(Korean Sentence Splitter)"
      ],
      "metadata": {
        "id": "PRySb0W-5PAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kss # pip install kss # 아나콘다에 없음\n",
        "\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
      ],
      "metadata": {
        "id": "B5jMRftd5cLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 한국어에서 토큰화의 어려움 : 어절\n",
        "- 교착어 : 형태소\n",
        "- 띄어쓰기의 자유\n",
        "6. 품사태깅\n",
        "\n",
        "7. 한국어 토큰화 실습"
      ],
      "metadata": {
        "id": "Tr0nq4Qo5kXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) \n",
        "\n",
        "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ],
      "metadata": {
        "id": "T9iZZ5Ld57vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02) 정제 Cleaning 과 정규화 Normalization\n",
        "\n",
        "- 정제 : 갖고 있는 코퍼스의 노이즈 제거\n",
        "- 정규화 : 표현방법이 다른 단어들을 통합시켜 같은 단어로 만듦\n",
        "\n",
        "1. 규칙에 기반한 표기가 다른 단어들 통합\n",
        "\n",
        "  USA, US -> USA\n",
        "\n",
        "2. 대, 소문자 통합\n",
        "\n",
        "3. 불필요한 단어 제거 : 빈도 적거나 길이 짧음 (ex. 불용어)\n",
        "\n",
        "4. 정규표현식 : 전처리에 이용\n",
        "\n",
        "### 03) 어간 추출 Stemming 과 표제어 추출 Lemmatization\n",
        "\n",
        "1. 표제어 추출 (기본 사전형 단어 is, are, am -> be)\n",
        "- 어간 : 의미 담은 부분\n",
        "- 접사 : 단어에 추가적인 의미줌"
      ],
      "metadata": {
        "id": "DOvlWmjT6Ao-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 :',words)\n",
        "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "id": "7Z-0VCio6iQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 어간 추출\n",
        "\n",
        "  어간 추출 : am -> am / having -> hav\n",
        "\n",
        "  표제어 추출 : am -> be / having -> have\n",
        "\n",
        "3. 한국어 어간 추출\n",
        "\n",
        "  어간 : 원칙적으로 모양이 바뀌지 않는 부분 (but 변하기도 함)\n",
        "\n",
        "  어미 : 용언의 어간에 붙어 활용하며 변하는 부분, 문법적 기능 \n",
        "  \n",
        "### 4) 불용어\n",
        "\n",
        "1. 불용어 확인"
      ],
      "metadata": {
        "id": "9rtM4sdx6k3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords # 불용어 리스트\n",
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "id": "TBKDBaij7FaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 불용어 제거"
      ],
      "metadata": {
        "id": "BviQvuj97MiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens: \n",
        "    if word not in stop_words: \n",
        "        result.append(word) \n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "id": "eX3I-yLH7QdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) 정규표현식"
      ],
      "metadata": {
        "id": "PvDpplGO7Tr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "RLzp-wON7XYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) 정수 인코딩\n",
        "1. dictionary 사용"
      ],
      "metadata": {
        "id": "pI9hgcFb7ZfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "sentences = sent_tokenize(raw_text) # 문장 토큰화\n",
        "\n",
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english')) # 불용어 목록\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence: \n",
        "        word = word.lower() # 단어 소문자화\n",
        "        if word not in stop_words: # 불용어 제거\n",
        "            if len(word) > 2: # 단어 길이가 2이하 제거 \n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1 # 단어 개수 구하기\n",
        "    preprocessed_sentences.append(result) \n",
        "\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "# np.hstack으로 문장 구분을 제거\n",
        "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
        "\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도순\n",
        "\n",
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)} # enumerate : 순서가 있는 자료형을 입력으로 받아 인덱스 순차적으로 함께 리턴\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "JyHbxRNQ7hii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 케라스 텍스트 전처리"
      ],
      "metadata": {
        "id": "jkUgOso67gpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences) # 빈도수 기준 단어 집합 생성\n",
        "\n",
        "print(tokenizer.word_index) # 인덱스\n",
        "print(tokenizer.word_counts) # 개수\n"
      ],
      "metadata": {
        "id": "UuN7VblW7n_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07) 패딩 Padding : 여러 문장의 길이 동일하게 유지\n",
        "\n",
        "1. Numpy 패딩"
      ],
      "metadata": {
        "id": "aiKBjLxz7q6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences) # 인덱스 대입\n",
        "print(encoded)\n",
        "\n",
        "max_len = max(len(item) for item in encoded) # 가장 긴 문장의 단어 수\n",
        "\n",
        "for sentence in encoded:\n",
        "    while len(sentence) < max_len:\n",
        "        sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np # 0 채우면 제로 패딩 zero padding"
      ],
      "metadata": {
        "id": "VZ3BDuw77yhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 케라스 전처리 도구 패딩"
      ],
      "metadata": {
        "id": "2OstcrMy7u1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(encoded) # 0을 앞에 채움\n",
        "padded = pad_sequences(encoded, padding='post') # 뒤\n"
      ],
      "metadata": {
        "id": "2M-LXHPX726N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) 원-핫 인코딩 One-Hot Encoding\n",
        "  단어 집합 vocabulary : 서로 다른 단어들의 집합\n",
        "\n",
        "1. 원-핫 인코딩 : 단어 집합의 크기를 벡터 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고 다른 인덱스에는 0을 부여하는 벡터 표현 방식\n",
        "\n",
        "  한계 : 단어의 유사성 판별 불가"
      ],
      "metadata": {
        "id": "W6K2y66I74w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()  \n",
        "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")  # 형태소\n",
        "\n",
        "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
        "\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector = [0]*(len(word_to_index))\n",
        "  index = word_to_index[word]\n",
        "  one_hot_vector[index] = 1\n",
        "  return one_hot_vector\n",
        "\n",
        "one_hot_encoding(\"자연어\", word_to_index)"
      ],
      "metadata": {
        "id": "pqZg08X68AQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9) 데이터의 분리\n",
        "\n",
        "1. 지도학습 Supervised Learning\n",
        "\n",
        "  X_train , y_train, X_test, y_train\n",
        "\n",
        "2. X와 Y 분리\n",
        "\n",
        "- zip 활용 분리 : 인덱스 0과 인덱스 1 분리"
      ],
      "metadata": {
        "id": "T3curI5B8LX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = [['a',1],['b',2],['c',3]]\n",
        "x, y =zip(*s)\n",
        "print(x,y)"
      ],
      "metadata": {
        "id": "_LHBC6gv9RE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터프레임으로 분리"
      ],
      "metadata": {
        "id": "FXFV7x7G8TLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
        "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
        "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
        "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
        "columns = ['메일 본문', '스팸 메일 유무']\n",
        "\n",
        "df = pd.DataFrame(values, columns=columns)\n",
        "\n",
        "X = df['메일 본문']\n",
        "y = df['스팸 메일 유무']\n",
        "\n",
        "# - numpy로 분리\n",
        "\n",
        "np_array = np.arange(0,16).reshape((4,4))\n",
        "\n",
        "X = np_array[:, :3]\n",
        "y = np_array[:,3]"
      ],
      "metadata": {
        "id": "tYD3BEbP8WeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 사이킷 런으로 테스트 데이터 분리"
      ],
      "metadata": {
        "id": "pZcreazi8Y9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234) # random_state : 데이터 다른 순서대로 섞임"
      ],
      "metadata": {
        "id": "95d45Cla8c3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10) 한국어 전처리 패키지\n",
        "\n",
        "1. PyKoSpacing : 띄어쓰기 딥 러닝 모델\n",
        "\n",
        "```\n",
        "! pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "```"
      ],
      "metadata": {
        "id": "ms55OlUP8fOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pykospacing import Spacing\n",
        "spacing = Spacing()\n",
        "\n",
        "kospacing_sent = spacing(new_sent) "
      ],
      "metadata": {
        "id": "cjyeSsjT8o2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Py-Hanspell : 맞춤법 검사 (띄어쓰기 보정 기능도 탑재)\n",
        "\n",
        "```\n",
        "! pip install git+https://github.com/ssut/py-hanspell.git\n",
        "```"
      ],
      "metadata": {
        "id": "DubA2h0A8raU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
        "spelled_sent = spell_checker.check(sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)"
      ],
      "metadata": {
        "id": "xKw9QuNJ8xFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. SOYNLP : 단어토큰화 (신조어 문제 해결), 반복되는 문자 정제\n",
        "\n",
        "```\n",
        "! pip install soynlp\n",
        "```\n",
        "4. Customised KoNLPy : 사용자 사전 추가 가능"
      ],
      "metadata": {
        "id": "p6_IwPNv80Gr"
      }
    }
  ]
}