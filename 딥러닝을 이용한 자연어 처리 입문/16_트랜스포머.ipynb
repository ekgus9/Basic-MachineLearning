{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16_트랜스포머.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/WM1lneaQzQ/n8jxI3RN2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 16. 트랜스포머 Transformer\n",
        "\n",
        "구글 발표 논문인 'Attention is all you need'에서 나온 모델\n",
        "\n",
        "1. 기존 seq2seq 모델의 한계\n",
        "\n",
        "  인코더 : 입력 시퀀스를 하나의 벡터 표현으로 압축\n",
        "\n",
        "  디코더 : 이 벡터 표현을 통해 출력 시퀀스를 만듦\n",
        "\n",
        "  -> 단점 : 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스 일부 손실\n",
        "\n",
        "  -> 트랜스포머 : 어텐션을 RNN 보정 용도가 아닌 어텐션 만으로 인코더와 디코더 구현\n",
        "\n",
        "2. 트랜스포머의 주요 하이퍼파라미터\n",
        "\n",
        "  dmodel = 512 : 트랜스포머의 인코더와 디코더에서 정해진 입력과 출력의 크기\n",
        "\n",
        "  num_layer = 6 : 하나의 인코더와 디코더를 층으로 생각했을 때, 인코더와 디코더가 총 몇 층으로 구성되었는지\n",
        "\n",
        "  num_heads = 8 : 트랜스포머에서 어텐션을 사용할 때 한 번에 하는 것보다 여러 개로 분할하여 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택함. 이때 병렬의 개수\n",
        "\n",
        "  dff = 2048 : 트랜스포머 내부 피드 포워드 신경망의 은닉층 크기\n",
        "\n",
        "3. 트랜스포머\n",
        "\n",
        "  RNN은 사용되지 않지만, 인코더-디코더 구조 유지\n",
        "\n",
        "4. 포지셔널 인코딩 Positional Encoding\n",
        "\n",
        "  트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줘야 한다. 즉 각 단어의 임베딩 벡터에 위치 정보를 더하여 모델 입력으로 사용하는 **포지셔널 인코딩**을 사용한다.\n",
        "\n",
        "5. 어텐션\n",
        "\n",
        "  인코더의 셀프 어텐션 : Query = Key = Value (벡터 출처가 같다.)\n",
        "\n",
        "  디코더의 마스크드 셀프 어텐션 : Query = Key = Value\n",
        "\n",
        "  디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터\n",
        "\n",
        "6. 인코더\n",
        "\n",
        "  num_layers 개의 인코더 층\n",
        "\n",
        "  하나의 인코더 층은 2개의 서브층으로 나뉜다. -> 셀프 어텐션, 피드 포워드 신경망\n",
        "\n",
        "7. 인코더의 셀프 어텐션\n",
        "\n",
        "  셀프 어텐션은 입력 문장 내 단어들끼리 유사도 구한다. \n",
        "  \n",
        "  -> '그 동물은 갈을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.' 에서 '그것'이 '동물'임을 특정할 수 있게 해준다.\n",
        "\n",
        "8. 포지션-와이즈 피드 포워드 신경망\n",
        "\n",
        "  인코더와 디코더에서 공통으로 가진 서브층\n",
        "\n",
        "12. 인코더에서 디코더로\n",
        "\n",
        "13. 디코더의 첫번째 서브층 : 셀프 어텐션과 룩-어헤드 마스크\n",
        "\n",
        "  현재 시점 단어를 예측하고자 할 때 미래 시점의 단어까지 참고하고자 하는 형상 발생 -> 이를 못하도록 룩-어헤드 마스크 도입\n",
        "\n",
        "14. 디코더의 두번째 서브층 : 인코더-디코더 어텐션\n",
        "\n",
        "  셀프 어텐션이 아님\n",
        "\n"
      ],
      "metadata": {
        "id": "5uoHBMd2BM_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9W4ebnjTBNIH"
      }
    }
  ]
}