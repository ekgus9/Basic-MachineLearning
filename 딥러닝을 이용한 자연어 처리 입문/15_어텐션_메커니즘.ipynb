{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_어텐션_메커니즘.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5CoQQmUwamww12U4gK1iH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 15. 어텐션 메커니즘 Attention Mechanism\n",
        "\n",
        "어텐션 : 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것 보정\n",
        "\n",
        "1. 어텐션의 아이디어\n",
        "\n",
        "  디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서 전체 입력 문장을 다시 한 번 참고\n",
        "\n",
        "2. 어텐션 함수\n",
        "\n",
        "  Attention(Q, K, V) = Attention Value\n",
        "\n",
        "  Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n",
        "  \n",
        "  K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "  \n",
        "  V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "\n",
        "  어텐션 값 : 주어진 쿼리에 대해 모든 키와의 유사도를 구한 뒤 각각 값에 반영하여 리턴\n",
        "\n",
        "3. 어텐션 종류\n",
        "\n",
        "  닷-프로덕트, 바다나우, ...\n",
        "\n",
        "3. 양방향 LSTM과 어텐션 메커니즘"
      ],
      "metadata": {
        "id": "ELoGvPCLMINW"
      }
    }
  ]
}